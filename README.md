# ğŸŒ Wikipedia Web Scraping

## ğŸ“Œ Overview
This project scrapes data from **Wikipedia** using **BeautifulSoup**, and **Requests**. The script extracts useful information from Wikipedia pages, processes the data, and stores it in structured formats for analysis.

## ğŸ›  Technologies Used
- **Python** ğŸ
- **BeautifulSoup** ğŸŒ (for parsing HTML)
- **Requests** ğŸ“¡ (for fetching Wikipedia pages)

## ğŸ“ˆ Key Features
- Scrapes Wikipedia pages for text, tables, and links.
- Cleans and structures the extracted data.
- Saves data in **Text** formats.
- Implements request handling and rate-limiting to avoid blocking.

## ğŸš€ Getting Started
### 1ï¸âƒ£ Clone the Repository
```bash
git clone https://github.com/Magnus0969/Wikipedia-Web-Scraping.git
cd Wikipedia-Web-Scraping
```

### 2ï¸âƒ£ Install Dependencies
```bash
pip install -r requirements.txt
```

### 3ï¸âƒ£ Run the Scraper
To scrape a Wikipedia page:
```bash
python scrape_wikipedia.py "https://en.wikipedia.org/wiki/Web_scraping"
```

## ğŸ“‚ Output
- Extracted text is saved in `Satoshi Nakamoto.txt`.

## ğŸ“ Example Extracted Data
**Sample Output from Wikipedia:**
```
Title: Web Scraping
First Paragraph: Web scraping, web harvesting, or web data extraction is data scraping used for extracting data from websites...
Links: ['https://en.wikipedia.org/wiki/Data_extraction', 'https://en.wikipedia.org/wiki/HTTP']
```

## ğŸ“¬ Contact
For any questions or collaborations, feel free to reach out on **[LinkedIn](https://www.linkedin.com/in/kmagadi/)**.

---
<p align="center">Made with â¤ï¸ by Karthik B Magadi</p>
