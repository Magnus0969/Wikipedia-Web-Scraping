# 🌐 Wikipedia Web Scraping

## 📌 Overview
This project scrapes data from **Wikipedia** using **BeautifulSoup**, and **Requests**. The script extracts useful information from Wikipedia pages, processes the data, and stores it in structured formats for analysis.

## 🛠 Technologies Used
- **Python** 🐍
- **BeautifulSoup** 🌐 (for parsing HTML)
- **Requests** 📡 (for fetching Wikipedia pages)

## 📈 Key Features
- Scrapes Wikipedia pages for text, tables, and links.
- Cleans and structures the extracted data.
- Saves data in **Text** formats.
- Implements request handling and rate-limiting to avoid blocking.

## 🚀 Getting Started
### 1️⃣ Clone the Repository
```bash
git clone https://github.com/Magnus0969/Wikipedia-Web-Scraping.git
cd Wikipedia-Web-Scraping
```

### 2️⃣ Install Dependencies
```bash
pip install -r requirements.txt
```

### 3️⃣ Run the Scraper
To scrape a Wikipedia page:
```bash
python scrape_wikipedia.py "https://en.wikipedia.org/wiki/Web_scraping"
```

## 📂 Output
- Extracted text is saved in `Satoshi Nakamoto.txt`.

## 📝 Example Extracted Data
**Sample Output from Wikipedia:**
```
Title: Web Scraping
First Paragraph: Web scraping, web harvesting, or web data extraction is data scraping used for extracting data from websites...
Links: ['https://en.wikipedia.org/wiki/Data_extraction', 'https://en.wikipedia.org/wiki/HTTP']
```

## 📬 Contact
For any questions or collaborations, feel free to reach out on **[LinkedIn](https://www.linkedin.com/in/kmagadi/)**.

---
<p align="center">Made with ❤️ by Karthik B Magadi</p>
